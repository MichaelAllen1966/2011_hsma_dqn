{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning introduction with CartPole\n",
    "\n",
    "### RL involves:\n",
    "* Trial and error search\n",
    "* Receiving and maximising reward (often delayed)\n",
    "* Linking state -> action -> reward\n",
    "* Must be able to sense something of their environment\n",
    "* Involves uncertainty in sensing and linking action to reward\n",
    "* Learning -> improved choice of actions over time\n",
    "* All models find a way to balance best predicted action vs. exploration\n",
    "\n",
    "### Elements of RL\n",
    "* *Environment*: all observable and unobservable information relevant to us\n",
    "* *Observation*: sensing the environment\n",
    "* *State*: the perceived (or perceivable) environment \n",
    "* *Agent*: senses environment, decides on action, receives and monitors rewards\n",
    "* *Action*: may be discrete (e.g. turn left) or continuous (accelerator pedal)\n",
    "* *Policy* (how to link state to action; often based on probabilities)\n",
    "* *Reward signal*: aim is to accumulate maximum reward over time\n",
    "* *Value function* of a state: prediction of likely/possible long-term reward\n",
    "* *Q*: prediction of likely/possible long-term reward of an *action*\n",
    "* *Advantage*: The difference in Q between actions in a given state (sums to zero for all actions)\n",
    "* *Model* (optional): a simulation of the environment\n",
    "\n",
    "### Types of model\n",
    "\n",
    "* *Model-based*: have model of environment (e.g. a board game)\n",
    "* *Model-free*: used when environment not fully known\n",
    "* *Policy-based*: identify best policy directly\n",
    "* *Value-based*: estimate value of a decision\n",
    "* *Off-policy*: can learn from historic data from other agent\n",
    "* *On-policy*: requires active learning from current decisions\n",
    "\n",
    "\n",
    "## Deep Q Networks for Reinforcement Learning\n",
    "\n",
    "Q = The expected future rewards discounted over time. This is what we are trying to maximise.\n",
    "\n",
    "The aim is to teach a network to take the current state observations and recommend the action with greatest Q.\n",
    "\n",
    "<img src=\"./images/dqn.png\" width=\"500\"/>\n",
    "\n",
    "Q is learned through the Bellman equation, where the Q of any state and action is the immediate reward achieved + the discounted maximum Q value (the best action taken) of next best action, where gamma is the discount rate.\n",
    "\n",
    "$$Q(s,a)=r + \\gamma.maxQ(s',a')$$\n",
    "\n",
    "## Key DQN components\n",
    "\n",
    "<img src=\"./images/dqn_components.png\" width=\"600|\"/>\n",
    "\n",
    "\n",
    "## General method for Q learning:\n",
    "\n",
    "Overall aim is to create a neural network that predicts Q. Improvement comes from improved accuracy in predicting 'current' understood Q, and in revealing more about Q as knowledge is gained (some rewards only discovered after time).\n",
    "\n",
    "<img src=\"./images/dqn_process.png\" width=\"600|\"/>\n",
    "    \n",
    "Target networks are used to stabilise models, and are only updated at intervals. Changes to Q values may lead to changes in closely related states (i.e. states close to the one we are in at the time) and as the network tries to correct for errors it can become unstable and suddenly lose signficiant performance. Target networks (e.g. to assess Q) are updated only infrequently (or gradually), so do not have this instability problem.\n",
    "\n",
    "## Training networks\n",
    "\n",
    "Double DQN contains two networks. This ammendment, from simple DQN, is to decouple training of Q for current state and target Q derived from next state which are closely correlated when comparing input features.\n",
    "\n",
    "The *policy network* is used to select action (action with best predicted Q) when playing the game.\n",
    "\n",
    "When training, the predicted best *action* (best predicted Q) is taken from the *policy network*, but the *policy network* is updated using the predicted Q value of the next state from the *target network* (which is updated from the policy network less frequently). So, when training, the action is selected using Q values from the *policy network*, but the the *policy network* is updated to better predict the Q value of that action from the *target network*. The *policy network* is copied across to the *target network* every *n* steps (e.g. 1000).\n",
    "\n",
    "<img src=\"./images/dqn_training.png\" width=\"700|\"/>\n",
    "\n",
    "## References\n",
    "\n",
    "van Hasselt H, Guez A, Silver D. (2015) Deep Reinforcement Learning with Double Q-learning. arXiv:150906461 [cs] http://arxiv.org/abs/1509.06461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code structure\n",
    "\n",
    "<img src=\"./images/dqn_program_structure.png\" width=\"700|\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           1 Import packages                                  #\n",
    "################################################################################\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           2 Define model parameters                          #\n",
    "################################################################################\n",
    "\n",
    "ENV_NAME = \"CartPole-v0\"\n",
    "DISPLAY_ON_SCREEN = False\n",
    "\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.003\n",
    "MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 10\n",
    "SYNC_TARGET_STEPS = 300\n",
    "REPLAY_START_SIZE = 500\n",
    "\n",
    "# Exploration rate (episolon) is probability of choosign a random action\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.001\n",
    "EXPLORATION_DECAY = 0.99\n",
    "TARGET_REWARD = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                 3 Define DQN (Duelling Deep Q Network) class                 #\n",
    "#                    (Used for both policy and target nets)                    #\n",
    "################################################################################\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    \"\"\"Deep Q Network solver\"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, neurons_per_layer=24):\n",
    "        \"\"\"Constructor method. Set up memory and neural nets.\"\"\"\n",
    "\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        # Set up memory for state/action/reward/next_state/done\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        \n",
    "        \n",
    "        # Set up network with two hidden layers\n",
    "        \n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(observation_space, neurons_per_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(neurons_per_layer, neurons_per_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(neurons_per_layer, action_space)\n",
    "            )\n",
    "        \n",
    "        # USe MSELoss as network predicting value (Q)\n",
    "        self.objective = nn.MSELoss()\n",
    "        \n",
    "        # Use Adam optimizer\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.net.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Act either randomly or by redicting action that gives max Q\"\"\"\n",
    "        # Act randomly if random number < exploration rate\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        # Otherwise choose best predicted outcome\n",
    "        q_values = self.net(torch.FloatTensor(state))\n",
    "        action = np.argmax(q_values.detach().numpy()[0]) \n",
    "        \n",
    "        return action\n",
    "    \n",
    "  \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                    4 Define policy net training function                     #\n",
    "################################################################################\n",
    "\n",
    "def optimize(policy_net, target_net, memory):\n",
    "    \"\"\"\n",
    "    Train update TensorFlow model by sampling from memory.\n",
    "    Uses policy network to predict Q.\n",
    "    Uses target network to prvide target of Q for next state/action.\n",
    "    Updates policy network against target network for sate/action.\"\"\"\n",
    "    \n",
    "    \n",
    "    # Do not try to train model if memory is less than reqired batch size\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return    \n",
    " \n",
    "    # Reduce exploration rate\n",
    "    policy_net.exploration_rate *= EXPLORATION_DECAY\n",
    "    policy_net.exploration_rate = max(EXPLORATION_MIN, \n",
    "                                      policy_net.exploration_rate)\n",
    "    # Sample a random batch from memory\n",
    "    batch = random.sample(memory, BATCH_SIZE)\n",
    "    for state, action, reward, state_next, terminal in batch:\n",
    "        \n",
    "        state_action_values = policy_net(torch.FloatTensor(state))\n",
    "       \n",
    "        if not terminal:\n",
    "            # For non-terminal actions get Q\n",
    "            expected_state_action_values = policy_net(torch.FloatTensor(state))\n",
    "            # Detach next state values from gradients to prevent updates\n",
    "            expected_state_action_values = expected_state_action_values.detach()\n",
    "            # Get nest state action with best Q from the policy net (double DQN)\n",
    "            policy_next_state_values = policy_net(torch.FloatTensor(state_next))\n",
    "            policy_next_state_values = policy_next_state_values.detach()\n",
    "            best_action = np.argmax(policy_next_state_values[0].numpy())\n",
    "            # Get targen net next state\n",
    "            next_state_action_values = target_net(torch.FloatTensor(state_next))\n",
    "            # Use detach again to prevent gradients from being updated\n",
    "            next_state_action_values = next_state_action_values.detach()\n",
    "            best_next_q = next_state_action_values[0][best_action].numpy()\n",
    "            updated_q = reward + (GAMMA * best_next_q)      \n",
    "            expected_state_action_values[0][action] = updated_q\n",
    "        else:\n",
    "            # For termal actions Q = reward (-1)\n",
    "            expected_state_action_values = policy_net(torch.FloatTensor(state))\n",
    "            # Detach values from gradients to prevent gradient update\n",
    "            expected_state_action_values = expected_state_action_values.detach()\n",
    "            # Set Q for all actions to reward (-1)\n",
    "            expected_state_action_values[0] = reward\n",
    "\n",
    "        # Update neural net\n",
    "        policy_net.optimizer.zero_grad()  \n",
    "        loss_v = nn.MSELoss()(state_action_values, \n",
    "                            expected_state_action_values)\n",
    "    \n",
    "        loss_v.backward()\n",
    "        policy_net.optimizer.step()\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                            5 Define memory class                             #\n",
    "################################################################################\n",
    "\n",
    "class Memory():\n",
    "    \"\"\"\n",
    "    Replay memomry used to train model.\n",
    "    Limited length memory (using deque, double ended queue from collections).\n",
    "    Holds, state, action, reward, next state, and episode done.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor method to initialise replay memory\"\"\"\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"state/action/reward/next_state/done\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                       6  Define results plotting function                    #\n",
    "################################################################################\n",
    "\n",
    "def plot_results(exploration, score):\n",
    "    \n",
    "    # Set up chart (ax1 and ax2 share x-axis to combine two plots on one graph)\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plot results\n",
    "    ax1.plot(exploration, label='exploration', color='g')\n",
    "    ax2.plot(score, label='score reward', color='r')\n",
    "    \n",
    "    # Set axes\n",
    "    ax1.set_xlabel('run')\n",
    "    ax1.set_ylabel('exploration', color='g')\n",
    "    ax2.set_ylabel('average reward', color='r')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 98, exploration 0.001, score 200                                            "
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "#                                 7 Main program                               #\n",
    "################################################################################\n",
    "\n",
    "    ########################################################################\n",
    "    #                          8 Set up environment                        #\n",
    "    ########################################################################\n",
    "\n",
    "# Set up game environemnt\n",
    "env = gym.make(ENV_NAME)\n",
    "# Get number of observations returned for state\n",
    "observation_space = env.observation_space.shape[0]\n",
    "# Get number of actions possible\n",
    "action_space = env.action_space.n\n",
    "\n",
    "    ########################################################################\n",
    "    #                    9 Set up policy and target nets                   #\n",
    "    ########################################################################\n",
    "\n",
    "# Set up neural nets\n",
    "policy_net = DQN(observation_space, action_space)\n",
    "target_net = DQN(observation_space, action_space)\n",
    "# Copy weights from policy_net to target\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "# Set target net to eval rather than training mode\n",
    "target_net.eval()    \n",
    "\n",
    "    ########################################################################\n",
    "    #                            10 Set up memory                          #\n",
    "    ########################################################################\n",
    "    \n",
    "# Set up memomry\n",
    "memory = Memory()\n",
    "\n",
    "    ########################################################################\n",
    "    #                     11 Set up + start training loop                  #\n",
    "    ########################################################################\n",
    "    \n",
    "# Set up run counter and start loop    \n",
    "run = 0; all_steps = 0\n",
    "# Set up results lists\n",
    "exploration_list = []\n",
    "score_list = []\n",
    "\n",
    "continue_learning = True\n",
    "while continue_learning:\n",
    "    \n",
    "    ########################################################################\n",
    "    #                           12 Play episode                            #\n",
    "    ########################################################################\n",
    "        \n",
    "    run += 1\n",
    "    \n",
    "    ########################################################################\n",
    "    #                             13 Reset game                            #\n",
    "    ########################################################################\n",
    "        \n",
    "    # Start run and get first state observations\n",
    "    state = env.reset()\n",
    "    # Reshape state into 2D array with state obsverations as first 'row'\n",
    "    state = np.reshape(state, [1, observation_space])\n",
    "    # Reset step count for episode\n",
    "    step = 0    \n",
    "\n",
    "    # Continue loop until episode complete\n",
    "    while True:\n",
    "        \n",
    "        ####################################################################\n",
    "        #                       14 Game episode loop                       #\n",
    "        ####################################################################\n",
    "        \n",
    "        # Incrememnt step coints\n",
    "        step += 1\n",
    "        all_steps += 1\n",
    "        # Option rander to screen\n",
    "        if DISPLAY_ON_SCREEN:\n",
    "            env.render()\n",
    "            \n",
    "        ####################################################################\n",
    "        #                       15 Get action                              #\n",
    "        ####################################################################\n",
    "        \n",
    "        # Get action to take\n",
    "        action = policy_net.act(state)\n",
    "        \n",
    "        ####################################################################\n",
    "        #                 16 Play action (get S', R, T)                    #\n",
    "        ####################################################################\n",
    "            \n",
    "        # Act\n",
    "        state_next, reward, terminal, info = env.step(action)\n",
    "        # Update display if needed\n",
    "        if DISPLAY_ON_SCREEN:\n",
    "            env.render()\n",
    "        # Convert step reward to negative if end of run\n",
    "        reward = reward if not terminal else -reward\n",
    "        # Get observations for new state\n",
    "        state_next = np.reshape(state_next, [1, observation_space])\n",
    "        \n",
    "        ####################################################################\n",
    "        #                  17 Add S/A/R/S/T to memory                      #\n",
    "        ####################################################################        \n",
    "        \n",
    "        # Record state, action, reward, new state & terminal\n",
    "        memory.remember(state, action, reward, state_next, terminal)\n",
    "        # Update state\n",
    "        state = state_next\n",
    "        \n",
    "        ####################################################################\n",
    "        #                  18 Check for end of episode                     #\n",
    "        ####################################################################\n",
    "            \n",
    "        # Actions to take if end of game episode\n",
    "        if terminal:\n",
    "            clear_line = ' ' * 80\n",
    "            print (f'\\r{clear_line}', end='')\n",
    "            print(f\"\\rRun {run}\" +\n",
    "                  f\", exploration {policy_net.exploration_rate:3.3f}, \" +\n",
    "                  f\"score {step}\", end='')\n",
    "            exploration_list.append(policy_net.exploration_rate)\n",
    "            score_list.append(step)\n",
    "            \n",
    "            ################################################################\n",
    "            #             18b Check for end of learning                    #\n",
    "            ################################################################\n",
    "            \n",
    "            if step >= TARGET_REWARD:\n",
    "                continue_learning = False\n",
    "            \n",
    "            # End current episode    \n",
    "            break\n",
    "\n",
    "        ####################################################################\n",
    "        #                        19 Update policy net                      #\n",
    "        ####################################################################\n",
    "            \n",
    "        # Avoid training model if memory is not of sufficient length\n",
    "        if len(memory.memory) < REPLAY_START_SIZE:\n",
    "            continue\n",
    "\n",
    "        # Update policy net \n",
    "        optimize(policy_net, target_net, memory.memory)\n",
    "        \n",
    "        ################################################################\n",
    "        #             20 Update target net periodically                #\n",
    "        ################################################################\n",
    "\n",
    "        # Update the target network at intervals\n",
    "        if all_steps % SYNC_TARGET_STEPS == 0:\n",
    "             target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot exploration and score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(exploration_list, score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model 10 times (we stopped the model as soon as it reached 200 steps - it is possible/likely this has not yet been optimally trained to achieve a score of 200 steps every single run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 194.6\n",
      "Stdev: 14.9\n",
      "Minumum: 150\n",
      "Maximum: 200\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()    \n",
    "totals = []\n",
    "for episode in range(10):\n",
    "    episode_reward = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        obs = np.reshape(obs, [1, observation_space])\n",
    "        action = policy_net.act(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        # Pole has fallen over if done is True\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_reward)\n",
    "    env.close()\n",
    "\n",
    "print (\"Average: {0:.1f}\".format(np.mean(totals)))    \n",
    "print (\"Stdev: {0:.1f}\".format(np.std(totals)))\n",
    "print (\"Minumum: {0:.0f}\".format(np.min(totals)))\n",
    "print (\"Maximum: {0:.0f}\".format(np.max(totals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe one run of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()    \n",
    "for step in range(200):\n",
    "    env.render()\n",
    "    obs = np.reshape(obs, [1, observation_space])   \n",
    "    action = policy_net.act(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    # Pole has fallen over if done is True\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
